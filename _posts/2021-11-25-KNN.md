---
layout: post
author: dinhhuyhoang
title: Bài 7 - K-Nearest Neighbors
---

**Phụ lục:**

- [1. Giới thiệu](#1-introduction)
- [2. Bài toán phân loại](#2-classifier)
    - [2.1. Ý tưởng](#21-idea)
    - [2.2. Thực nghiệm với Python](#22-coding)
- [3. Bài toán hồi quy](#3-regression)
    - [3.1. Ý tưởng](#31-idea)
    - [3.2. Thực nghiệm với Python](#32-coding)
- [4. Đánh giá và kết luận](#4-evaluation)
- [5. Tham khảo](#5-references)

<a name="1-introduction"></a>

## 1. Giới thiệu

KNN (K-Nearest Neighbors) là một thuật toán thuộc Supervised-learning. Ý tưởng để triển khai thuật toán này rất đơn giản. Thuật toán dựa trên 2 yếu tố chính đó là: similarity measure (khoảng cách) giữa các điểm dữ liệu và sử dụng những "hàng xóm" lân cận để dự đoán. Một điểm đặc biệt của thuật toán này đó là không có một hàm số giả định nào để học từ nó, khác với bài toán Linear Regression khi ta cần định nghĩa hàm số dự đoán là đường thẳng hoặc parabole. Hơn nữa, khi training thuật toán này sẽ không hề học gì từ dữ liệu, việc tính toán sẽ tập trung toàn bộ khi dự đoán một điểm dữ liệu mới.

Với những lí do nêu trên: KNN còn có thể gọi là <i>non-parameter method, Instance-based learning, Lazy Learning, Memory-based learning</i>. Thuật toán KNN có thể sử dụng cho 2 lớp bài toán hồi quy dự đoán (regression) và phân loại (classification) trong Supervised-leaning.

<a name="2-classifier"></a>

## 2. Bài toán phân loại

<a name="21-idea"></a>

### 2.1. Ý tưởng

Giả sử ta có tập dữ liệu như sau, 3 nhãn: 0 tương ứng hình tròn - màu đỏ, 1 tương ứng hình tam giác - màu xanh lục, 2 tương ứng hình vuông - màu xanh dương và điểm dữ liệu x - màu tím cần dự đoán thuộc nhãn nào trong 3 nhãn trên.

<img src="/assets/images/bai7/anh1.png" class="normalpic"/>

<p align="center"> <b>Hình 1</b>: Dữ liệu</p>

Ý tưởng để tìm nhãn cho dữ liệu x - màu tìm rất đơn giản.

- Bước 1: Tính toán khoảng cách giữa x với toàn bộ điểm dữ liệu

- Bước 2: Chọn ra $K$ điểm có khoảng cách ngắn nhất

- Bước 3: Với $K$ điểm tìm được ở bước 2, lấy ra $a$ - số lượng khoảng cách của x so với nhãn 0, tương tự $b$ - nhãn 1 và $c$ - nhãn 2. Ta có: $a + b + c = K$.

- Bước 4: Tìm $max(a,b,c)$. Ví dụ nếu $max(a,b,c) = a$ thì dữ liệu x - màu tìm sẽ thuộc nhãn 0. Kỹ thuật này còn được gọi là <i>major voting</i>.

<p style="display: flex">
<img src="/assets/images/bai7/anh2.png" class="smallpic"/> <img src="/assets/images/bai7/anh3.png" class="smallpic"/>
</p>
<p align="center"> <b>Hình 2</b>: Bên trái - các đường thẳng màu vàng là khoảng cách giữa điểm dữ liệu x với toàn bộ dữ liệu. Bên phải - $K$ = 4 ta được 4 đường thẳng ngắn nhất so với x, lúc này số lượng đường thẳng so với x thuộc nhãn 1 (hình tam giác - màu xanh lục) là max. Vì vậy điểm x sẽ thuộc nhãn 1. </p>

<a name="22-coding"></a>

### 2.2. Thực nghiệm với Python

Ở phần này, mình sẽ sử dụng một bộ thư viện có sẵn trong sklearn đó là Iris (một bộ dữ liệu về các loài hoa). Bộ dữ liệu gồm 3 nhãn: Iris setosa, Iris virginica và Iris versicolor. Mỗi nhãn có 50 bông hoa với 4 thông tin chính: chiều dài, chiều rộng đài hoa (sepal), và chiều dài, chiều rộng cánh hoa (petal). Ảnh cho mỗi loài hoa:

<img src="/assets/images/bai7/anh4.png" class="normalpic"/>

<p align="center"> <b>Hình 3</b>: Iris dataset (nguồn: <a href="https://machinelearningcoban.com/2017/01/08/knn/">Machine learning cơ bản</a>)</p>

Load dữ liệu

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()

X = iris.data
y = iris.target

print('Number of classes: %d' %len(np.unique(y)))
>>> Number of classes: 3

print('Number of data points: %d' %X.shape[0])
>>> Number of data points: 150

print('#Features of each point: %d'%X.shape[1])
>>> Features of each point: 4
```

Tiếp theo chia bộ dataset thành training set (2/3) và test set (1/3). Tức ta sẽ lấy kết quả ở test set làm kết quả của bài toán, ở trong bộ dữ liệu này phân phối mỗi nhãn là tương đồng nhau nên ta có thể sử dụng accuracy score để đánh giá. 

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
     X, y, test_size=50)

print("Numbers of training set: %d" %X_train.shape[0])
>>> Numbers of training set: 100

print("Numbers of test set: %d" %X_test.shape[0])
>>> Numbers of test set: 50
```

Tiếp theo ta sẽ xây dựng các hàm cần thiết cho bài toán.

- Hàm tính khoảng cách giữa 2 điểm

```python
# Khoảng cách o-clit giữa 2 điểm
def distance(p1,p2):
    return np.linalg.norm(p1-p2,2)
```

- Hàm lấy ra toàn bộ khoảng cách với điểm xét

```python
# Tính toàn bộ khoảng cách với điểm xét x0
def get_all_distance(x0,X,y):
    distances = []
    for i in range(len(X)):
        dist = distance(x0,X[i])
        distances.append((dist,y[i]))
    return distances
```

- Hàm lấy K điểm gần nhất

```python
# Lấy ra K điểm có khoảng cách ngắn nhất
def get_K_nearest_neighbors(K,X,y,x0):
    neighbors = []
    distances = get_all_distance(x0,X,y)
    distances.sort(key=operator.itemgetter(0))
    for i in range(K):
        neighbors.append(distances[i][1])
    return neighbors
```

- Hàm lấy ra nhãn có lượng vote cao nhất

```python
# Lấy ra nhãn có số vote cao nhất
def major_voting(neighbors,n_classes):
    counting = [0]*n_classes
    for i in range(n_classes):
        counting[neighbors[i]] += 1
    return np.argmax(counting)
```

- Hàm dự đoán và hàm tính số phần trăm chính xác

```python
# Hàm dự đoán
def predict(X,y,K,x0,n_classes):
    neighbors = get_K_nearest_neighbors(K,X,y,x0)
    y_pred = major_voting(neighbors,n_classes)
    return y_pred

# Tính phần trăm chính xác của dự đoán và test
def accuracy_score(y_preds,y_test):
    n = len(y_preds)
    count = 0
    for i in range(n):
        if y_preds[i] == y_test[i]:
            count += 1
    return count/n
```

- Hàm main

```python
def main():
    K = 5
    n_classes = 3
    y_preds = []
    for xt in X_test:
        yt = predict(X_train, y_train, K, xt,n_classes)
        y_preds.append(yt)
    acc = accuracy_score(y_preds,y_test)
    print('Accuracy score: %f' %acc)

if __name__ == '__main__':
    main()
```

Kết quả thu được sau vài lần chạy thường đặt được accuracy khá cao: 0.96, 0.98 thậm chí đôi khi lên tới 1.0.

<a name="3-regression"></a>

## 3. Bài toán hồi quy

<a name="31-idea"></a>

### 3.1. Ý tưởng

Ý tưởng với bài toán hồi quy cũng rất đơn giản. Giả sử ta có một tập dữ liệu như sau về thời gian học và số điểm tương ứng.

<img src="/assets/images/bai7/anh5.png" class="normalpic"/>

<p align="center"> <b>Hình 4</b>: Dữ liệu ví dụ</p>

Lúc này với các điểm hình tròn - màu đỏ là các dữ liệu mẫu đã có, ta cần dự đoán với điểm dữ liệu tam giác - màu xanh lục sẽ có giá trị score tương ứng là bao nhiêu với time. Ý tưởng của thuật toán KNN với bài toán hồi quy như sau:
- Bước 1: Tính khoảng cách của dữ liệu xét với toàn bộ dữ liệu mẫu.

- Bước 2: Lấy ra K dữ liệu mẫu có K khoảng cách ngắn nhất.

- Bước 3: Giá trị dự đoán là trung bình của biến mục tiêu với K dữ liệu mẫu tương ứng tìm được ở trên.

<p style="display: flex">
<img src="/assets/images/bai7/anh6.png" class="smallpic"/> <img src="/assets/images/bai7/anh7.png" class="smallpic"/>
</p>
<p align="center"> <b>Hình 5</b>: Bên trái - Sau khí tính toàn bộ khoảng cách, với $K$ = 3 ta lấy được 3 điểm hình tròn (lúc này màu xanh lục) là 3 giá trị có khoảng cách ngắn nhất so với điểm dữ liệu đang xét. Bên phải - Lấy trung bình các giá trị $y_1, y_2, y_3$ với 3 điểm trên ta được $y_0$ chính là giá trị cần dự đoán của $x_0$.</p>

<a name="32-coding"></a>

### 3.2. Thực nghiệm với Python

Vì việc code khá tương tự phần trên, nên ở phần này mình sẽ không code from scrath mà sử dụng thư viện sklearn và đánh giá với việc chọn $K$.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import neighbors

y = np.array([[2,5,7,9,11,16,19,23,25,29,29,35,37,35,40,42,39,31,30,28,20,15,10]],dtype='float').T
X = np.array([[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]],dtype='float').T

x0 = np.linspace(2,24,500).reshape(-1,1)
K = 3

knn = neighbors.KNeighborsClassifier(n_neighbors=K)
knn.fit(X,y)
y_predict = knn.predict(x0)

plt.plot(x0,y_predict)
plt.plot(X,y,'ro')
plt.title('K = %d'%K)
plt.show()
```

<img src="/assets/images/bai7/anh8.png" class="normalpic"/>

<p align="center"> <b>Hình 6</b>: Kết quả</p>

Với giá trị $K$ khác nhau ta có:

<p style="display: flex">
<img src="/assets/images/bai7/anh9.png" class="smallpic"/> <img src="/assets/images/bai7/anh10.png" class="smallpic"/>
</p>
<p style="display: flex">
<img src="/assets/images/bai7/anh11.png" class="smallpic"/> <img src="/assets/images/bai7/anh12.png" class="smallpic"/>
</p>

<p align="center"> <b>Hình 7</b>: So sánh các giá trị $K$</p>

<a name="4-evaluation"></a>

## 4. Đánh giá và kết luận

- Ta thấy với giá trị $K$ nhỏ, hầu như các giá trị dự đoán rất khít với giá trị mẫu, điều này dẫn tới việc dự đoán khi có dữ liệu nhiễu (outlier) sẽ làm thuật toán bị overfitting. Vì vậy thuật toán này sẽ không thường chọn giá trị $K$ quá nhỏ.

- Việc sử dụng major voting ở trên ta có thể sử dụng thêm trọng số về khoảng cách để tăng độ tin cậy (khoảng cách càng gần thì độ tin cậy càng cao). Ở trong sklearn có cung cấp attribute <i>weights = 'distance'</i> khi khởi tạo KNN. Đọc thêm <a href="https://machinelearningcoban.com/2017/01/08/knn/#danh-trong-so-cho-cac-diem-lan-can">tại đây</a>.

- Thuật toán KNN là thuật toán đơn giản và có thể mang lại độ chính xác khá cao, nhưng khi training một tập dữ liệu lớn thì rất chậm vì việc tính toán của KNN sẽ bắt đầu lại từ đầu khi có dữ liệu mới.

## 5. Tham khảo

[1] [Machine Learning cơ bản](https://machinelearningcoban.com/2017/01/08/knn/)

[2] [Machine Learning and Data Mining](https://users.soict.hust.edu.vn/khoattq/ml-dm-course/L6-KNN.pdf)