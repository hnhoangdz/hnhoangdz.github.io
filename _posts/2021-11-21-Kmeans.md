---
layout: post
author: dinhhuyhoang
title: Bài 7 - K-means
---

**Phụ lục:**

- [1. Giới thiệu](#1-introduction)
- [2. Kết quả mong muốn](#2-expedted)
- [3. Hàm mất mát](#3-loss)
- [4. Các bước giải bài toán](#4-steps)
- [5. Thực nghiệm với Python](#5-coding)
- [6. Đánh giá và kết luận](#6-evaluation)
- [7. Tham khảo](#7-references)

<a name="1-introduction"></a>

## 1. Giới thiệu

Trong thực tế, những bài toán như dự báo, phân loại yêu cầu phần lớn dữ liệu cần phải gán nhãn. Vậy nếu khi có một tập dữ liệu về khách hàng như: sở thích, thói quen, giới tính, độ tuổi... làm sao để phân loại khách hàng nào là tiềm năng hoặc không trong khi các dữ liệu chưa được gán nhãn. Việc đi tìm nhãn cho khách hàng (dữ liệu) chính là mục tiêu của thuật toán K-means. Và sau khi gán nhãn cho khách hàng thành công, khi một khách hàng mới đến ta có thể trả lời rằng người này thuộc nhóm khách hàng tiềm năng hay không, từ đó có thể phát triển rất nhiều chiến lược về khuyến mại và marketing.

Khác với các bài toán thuộc học có giám sát (supervised-learning) - các biến mục tiêu đã biết, K-means là lớp bài toán thuộc học không giám sát (unsupervised-learning) - các biến mục tiêu chưa biết.

<img src="/assets/images/bai7/anh1.png" class="normalpic"/>

<p align="center"> <b>Hình 1</b>: Supervised vs Unsupervised</p>

<a name="2-expected"></a>

## 2. Kết quả mong muốn 

Trong thuật toán K-means, với tập dữ liệu có $m$ samples. Ta cần phân nhóm $m$ samples thành $k$ $(k < m)$ cụm, với mỗi cụm sẽ có độ tương đồng về dữ liệu nhất định (có thể là sở thích, tích cách, giới tính... đối với người). Với **hình 1** bên phải, tức ta cần phân nhóm 9 samples thành 2 cụm: màu xanh lục và màu đỏ. 

Ví dụ với tập dữ liệu 2 chiều sau:

<p style="display: flex">
<img src="/assets/images/bai7/anh2.png" class="smallpic"/> <img src="/assets/images/bai7/anh3.png" class="smallpic"/>
</p>

<p align="center"> <b>Hình 2</b>: Hình trái - Dataset, Hình phải - Kết quả (<b>Nguồn: </b><a href="https://www.coursera.org/learn/machine-learning/lecture/93VPG/k-means-algorithm">Machine learning - Coursera</a>)</p>

Với input là tập dataset chưa hề có nhãn cho từng điểm dữ liệu, mong muốn của bài toán nhằm gán nhãn cho toàn bộ dữ liệu. Bằng trực giác ta có thể thấy rằng với **Hình 2 - bên phải** biểu thị kết quả rằng những điểm dữ liệu có khoảng cách càng gần nhau sẽ có màu càng giống nhau. Tuy nhiên, làm sao mà kết quả có thể chia làm 2 màu xanh, đỏ rõ ràng như vậy? Đây chính là mấu chốt của thuật toán này, nếu để ý 2 điểm 'x' màu đỏ và xanh có ở kết quả thì ban đầu dữ liệu không hề có 2 điểm này. Hơn nữa, nếu gióng tọa độ của 2 điểm này lên trục tọa độ ta sẽ thấy giá trị tọa độ của nó là trung bình cộng của toàn bộ điểm dữ liệu thuộc màu của nó. 2 điểm này chính là tâm cụm (centroids) của mỗi màu dữ liệu.

Giả sử rằng mỗi điểm dữ liệu chỉ thuộc vào đúng một nhóm, trong thuật toán K-means, ta cần lựa chọn số lượng cụm như là hyper-parameter đầu vào. Với hình trên thì ta có thể thấy số lượng cụm là 2 khá hợp lí. Vì vậy, từ dữ liệu đầu vào và số lượng nhóm chúng ta muốn tìm, hãy chỉ ra centroids (điểm trung tâm cụm) của mỗi nhóm và phân các điểm dữ liệu vào các nhóm tương ứng. 

<a name="3-loss"></a>

## 3. Hàm mất mát

Giả sử ta có $m$ điểm dữ liệu. $\mathbf{X} = \begin{bmatrix} x^{(1)}, x^{(2)}, x^{(3)},...x^{(m)} \end{bmatrix}$ trong đó $x^{(i)} \in \mathbb{R}^d $, $i$ chạy từ 1,2,...$m$, $\mathbf{X} \in \mathbb{R}^{d \times m}$. Có $K$ cụm với $K < m$. Đặt $c_k$ là tâm cụm ta đã tìm được và $x^{(i)}$ là điểm dữ liệu thuộc cụm này. Điều ta mong muốn là khoảng cách (sai số) giữa $c_k$ và $x_i$ là nhỏ nhất có thể. Và sai số trên toàn tập dữ liệu là:

$$J = \sum_{j=1}^K\sum_{i=1}^m \|\mathbf{x}^{(i)} \in j - \mathbf{c}_j\|_2^2 $$

Ở công thức trên ta hiểu là: tổng sai số bằng tổng khoảng cách của tâm cụm $j$ với các điểm dữ liệu $x_i$ thuộc cụm $j$. Trong đó $j$ được hiểu ngầm là nhãn cho dữ liệu vì vậy ta sẽ có $K$ nhãn khác nhau. Và $c_j$ là trung bình cộng của các điểm thuộc cụm $j$.

Ở đây, mình đã đơn giản hóa cách biểu diễn để khi thực code sẽ dễ dàng hơn, bạn có thể đọc thêm [tại đây](https://machinelearningcoban.com/2017/01/01/kmeans/#-phan-tich-toan-hoc) để xem cách biểu diễn dạng vector one-hot, chứng minh tâm cụm là trung bình cộng của các điểm dữ liệu thuộc cụm đó và hàm Loss luôn giảm sau mỗi vòng lặp. Ngoài ra thuật toán sẽ dừng sau 1 số hữu hạn vòng lặp.

<a name="4-steps"></a>

## 4. Các bước giải bài toán

- **Bước 1**: Chọn $K$ điểm dữ liệu làm điểm tâm cụm ban đầu. 

- **Bước 2**: Tính khoảng cách mỗi điểm dữ liệu với tâm cụm và gán dữ liệu vào cụm gần nhất.

- **Bước 3**: Nếu việc gán dữ liệu vào từng cluster ở bước 2 không thay đổi so với vòng lặp trước nó thì ta dừng thuật toán.

- **Bước 4**: Cập nhật điểm tâm cụm bằng trung bình cộng các dữ liệu thuộc cụm đó.

- **Bước 5**: Quay lại bước 2.

<a name="5-coding"><a>

## 5. Thực nghiệm với Python

### 5.1. Implement thuật toán

Đầu tiên ta sẽ tạo dataset, tuy nhiên lưu ý 2 biến `true_centroids` và `true_labels` trong thực tế sẽ không có.

```python
import numpy as np # ĐSTT
import random 
import matplotlib.pyplot as plt # Visualize
from sklearn.datasets import make_blobs # Make the dataset

# Init original centroids
true_centroids = [[1, 1], [-1, -1], [1, -1]] 

# dataset
X, true_labels = make_blobs(n_samples=750, centers=true_centroids, 
                            cluster_std=0.4, random_state=0)

# Visualize dữ liệu
plt.plot(X[:,0],X[:,1],'o')
plt.title('Dataset')
```

<img src="/assets/images/bai7/anh4.png" class="normalpic"/>

<p align="center"> <b>Hình 3</b>: Visualize dataset</p>

Giả sử với bộ dữ liệu trên, ta khởi tạo K = 3 là số cụm và sẽ thu các tâm cụm tương ứng với giá trị ngẫu nhiên.

```python
# Init random centroids
def init_centroids(K):
    centroids = []
    K = 3
    for i in range(K):
        x = random.sample(list(X), 1)
        centroids.append(x[0])
    centroids = np.array(centroids)
    return centroids

K = 3
centroids = init_centroids(K)
plt.plot(centroids[:,0],centroids[:,1],'o')
```

<img src="/assets/images/bai7/anh5.png" class="normalpic"/>

<p align="center"> <b>Hình 4</b>: Khởi tạo tâm cụm</p>

Tiếp theo là hàm tính toán khoảng cách giữa 2 điểm với K-means khoảng cách được sử dụng sẽ là norm 2.

```python
# Tính khoảng cách 2 điểm
def distance(p1,p2):
    return np.linalg.norm(p1-p2,2)
```

Tiếp theo ta cần tính toán khoảng cách của từng điểm dữ liệu với từng centroids, khoảng cách ngắn nhất của điểm dữ liệu với từng centroids sẽ được coi là nhãn của điểm dữ liệu đó.

```python
# Update labels for dataset
def update_labels(X):
    labels = []
    for i in range(len(X)):
        fake_distance = 999999
        label = -1
        for j in range(len(clusters)):
            d = distance(X[i],clusters[j])
            if d < fake_distance:
                fake_distance = d
                label = j
        labels.append(label)
    return labels
```

Sau khi đã lấy nhãn cho toàn bộ dữ liệu, ta cần cập nhật giá trị tâm cụm bằng cách tính trung bình cộng của những điểm dữ liệu thuộc nhãn của cụm.

```python
# Update centroids
def update_centroids(centroids, X, labels):
    before_centroids = centroids.copy()
    for i in range(len(centroids)):
        count = 0
        x0 = 0
        y0 = 0
        for j in range(len(X)):
            if i == labels[j]:
                count += 1
                x0 += X[j][0]
                y0 += X[j][1]
        x0 /= count
        y0 /= count
        centroids[i][0] = x0
        centroids[i][1] = y0
    return labels, centroids, before_centroids
```

Cuối cùng là điều kiện dừng của thuật toán, ở đây ta sét cho tới khi giá trị của centroids không đổi thì sẽ dừng.

```python
# When stop
def stop(centroids,new_centroids):
    return (set([tuple(a) for a in centroids]) == 
        set([tuple(a) for a in new_centroids]))
```

Các hàm quan trọng của bài toán đã được hoàn thành, bây giờ ta cần lắp ghép chúng vào theo phần 4 để tìm nghiệm cho bài toán. 

```python
while True:
    labels = update_labels(X)
    labels, centroids, before_centroids = update_centroids(centroids, X, labels)
    if stop(centroids, before_centroids):
        break
plt.scatter(X[:,0], X[:,1], c = labels, cmap='rainbow')
plt.scatter(centroids[:,0], centroids[:,1], color='black')
```

**Kết quả:** 

<img src="/assets/images/bai7/anh6.png" class="normalpic"/>

<p align="center"> <b>Hình 5</b>: Kết quả thu được</p>

**Nhận xét:**

- Sau 1 số vòng lặp, ta đã chia được dữ liệu thành 3 cụm khác nhau trông có vẻ được phân thành 3 nhóm khá rõ ràng. 

- Lúc này khi có điểm dữ liệu mới đến, ta chỉ cần tính toán khoảng cách giữa điểm đó với các tâm cụm để lấy ra khoảng cách nhỏ nhất chính là thuộc cụm đó. Phần này khá dễ, bạn có thể thử implement.

### 5.2. Nghiệm bằng thư viện scikit-learn

```python
from sklearn.cluster import KMeans

k = 3
kmeans = KMeans(n_clusters=k)
kmeans.fit(X)
centroids = kmeans.cluster_centers_

plt.scatter(X[:,0], X[:,1], c = labels, cmap='rainbow')
plt.scatter(centroids[:,0], centroids[:,1], color='black')
```

**Kết quả:**

<img src="/assets/images/bai7/anh7.png" class="normalpic"/>

<p align="center"> <b>Hình 6</b>: Kết quả thu được</p>

**Nhận xét:**

- Kết quả thu được của scikit-learn giống với hàm mà mình đã implement, các giá trị centroids và nhãn đã giống nhau. 

- Với scikit-learn, khi có dữ liệu mới ta chỉ cần gọi hàm `predict` để dự đoán xem điểm đó thuộc cụm nào (chú ý hàm này có thể dùng để dự đoán không chỉ một mà có thể là tập hợp các điểm dữ liệu). Ta nhận thấy rằng tuy thuật toán chạy quá trình training có thể mất thời gian một chút ($O(mn^2)$) với $m$ là số lần lặp của thuật toán, nhưng khi dự đoán thì chỉ mất ($O(k)$) với $k$ là số lượng cụm.

- Tuy nhiên, một điểm yếu của K-means là việc random các điểm centroids ban đầu sẽ ảnh hưởng tới kết quả của bài toán. Như ở phần implement 5.1, mình đã thử chạy vài lần ở mỗi lần số lượng vòng lặp sẽ khác nhau phụ thuộc vào điểm khởi tạo ban đầu thậm chí đôi khi nghiệm cuối cùng thu được không phải là nghiệm tối ưu của bài toán (local optima), bằng trực giác ta thấy rằng các điểm khởi tạo ban đầu có khoảng cách càng xa nhau càng tốt. Ví dụ minh họa:

<img src="/assets/images/bai7/anh8.png" class="normalpic"/>

<p align="center"> <b>Hình 7</b>: Điểm yếu của khởi tạo</p>

- Ngoài ra số lượng cụm cũng cần phải khởi tạo ban đầu, với bộ dữ liệu 2D ở trên thì khá dễ để chọn nhưng trong trường hợp dữ liệu ở dạng cao chiều sẽ không thể nhìn trực tiếp để chọn số lượng cụm. Vì vậy phần tiếp theo mình sẽ giới thiệu 1 số cách cải tiến những vấn đề này.

## 6. Cải tiến K-means

### 6.1. Khởi tạo

<a name="6-evaluation"></a>

## 6. Đánh giá và kết luận

- K-means là một thuật toán đơn giản, khá dễ để hiểu ý tưởng nhưng vẫn được ứng dụng khá nhiều trong các bài khác làm bước pre-processing. Hơn nữa, tuy thời gian để tính toán và suy ra nghiệm (tâm cụm) hơi tốn thời gian, nhưng bù lại khi dự đoán lại rất nhanh.

- Một số ứng dụng của K-means có thể kể tới là phân đoạn ảnh (Image Segmentation) một lĩnh vực rất hay trong Computer Vision hoặc Image Compression (nén ảnh).

- Điểm yếu của K-means đó là ta phải biết trước số cụm $K$ để phân thành $K$ dữ liệu khác nhau vì vậy có một phương pháp tên là Ebbow Method được sử dụng để chọn $K$ và thử nghiệm những giá trị $K$ phù hợp nhất có thể. 

<a name="7-references"></a>

## 7. Tham khảo

[1] [Machine Learning cơ bản](https://machinelearningcoban.com/2017/01/01/kmeans/)

[2] [Machine Learning course by Andrew Ng](https://www.coursera.org/learn/machine-learning)



