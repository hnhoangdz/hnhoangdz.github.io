---
layout: post
author: dinhhuyhoang
title: Bài 6 - K-means
---

**Phụ lục:**

- [1. Giới thiệu](#1-introduction)
- [2. Kết quả mong muốn](#2-expedted)
- [3. Hàm mất mát](#3-loss)
- [4. Các bước giải bài toán](#4-steps)
- [5. Thực nghiệm với Python](#5-coding)
- [6. Đánh giá và kết luận](#6-evaluation)
- [7. Tham khảo](#7-references)

<a name="1-introduction"></a>

## 1. Giới thiệu

Trong thực tế, những bài toán như dự báo, phân loại yêu cầu phần lớn dữ liệu cần phải gán nhãn. Vậy nếu khi có một tập dữ liệu về khách hàng như: sở thích, thói quen, giới tính, độ tuổi... làm sao để phân loại khách hàng nào là tiềm năng hoặc không trong khi các dữ liệu chưa được gán nhãn. Việc đi tìm nhãn cho khách hàng (dữ liệu) chính là mục tiêu của thuật toán K-means. Và sau khi gán nhãn cho khách hàng thành công, khi một khách hàng mới đến ta có thể trả lời rằng người này thuộc nhóm khách hàng tiềm năng hay không, từ đó có thể phát triển rất nhiều chiến lược về khuyến mại và marketing.

Khác với các bài toán thuộc học có giám sát (supervised-learning) - các biến mục tiêu đã biết, K-means là lớp bài toán thuộc học không giám sát (unsupervised-learning) - các biến mục tiêu chưa biết.

<img src="/assets/images/bai6/anh1.png" class="normalpic"/>

<p align="center"> <b>Hình 1</b>: Sự khác biệt giữa supervised và unsupervised</p>

<a name="2-expected"></a>

## 2. Kết quả mong muốn 

Trong thuật toán K-means, với tập dữ liệu có $m$ samples. Ta cần phân nhóm $m$ samples thành $k$ cụm, với mỗi cụm sẽ có độ tương đồng về dữ liệu nhất định (có thể là sở thích, tích cách...). Với hình 1 bên phải, tức ta cần phân nhóm 9 samples thành 2 cụm: màu xanh lục và màu đỏ. Giả sử với tập dữ liệu 2 chiều sau:

<img src="/assets/images/bai6/anh2.png" class="normalpic"/>

<p align="center"> <b>Hình 2</b>: Visualize dữ liệu (<b>Source: </b><a href="https://www.coursera.org/learn/machine-learning/lecture/93VPG/k-means-algorithm">Machine learning - Coursera</a>)</p>

Với thuật toán K-means, ta cần lựa chọn số lượng cụm như là hyper-parameter đầu vào. Với hình trên thì ta có thể thấy số lượng cụm là 2 khá hợp lí. Vì vậy, từ dữ liệu đầu vào và số lượng nhóm chúng ta muốn tìm, hãy chỉ ra center (điểm trung tâm cụm) của mỗi nhóm và phân các điểm dữ liệu vào các nhóm tương ứng. Giả sử thêm rằng mỗi điểm dữ liệu chỉ thuộc vào đúng một nhóm. Ta mong muốn kết quả như sau:

<img src="/assets/images/bai6/anh3.png" class="normalpic"/>

<p align="center"> <b>Hình 3</b>: Kết quả</p>

Như hình trên, ta thấy 2 cụm xanh và đỏ riêng biệt đã được phân riêng rẽ. Hơn nữa 2 điểm trung tâm (center cluster) - kí hiệu 'x' là tâm mỗi cụm.

<a name="3-loss"></a>

## 3. Hàm mất mát

Giả sử ta có $m$ điểm dữ liệu. $\mathbf{X} = \begin{bmatrix} x^{(1)}, x^{(2)}, x^{(3)},...x^{(m)} \end{bmatrix}$ trong đó $x^{(i)} \in \mathbb{R}^d $, $i$ chạy từ 1,2,...$m$ và $\mathbf{X} \in \mathbb{R}^{d \times m}$. Có $K$ cụm với $K < m$. Đặt $c_k$ là tâm cụm ta đã tìm được và $x^{(i)}$ là điểm dữ liệu thuộc cụm này. Điều ta mong muốn là khoảng cách (sai số) giữa $c_k$ và $x_i$ là nhỏ nhất có thể. Và sai số trên toàn tập dữ liệu là. 

$$J = \sum_{j=1}^K\sum_{i=1}^m \|\mathbf{x}^i:j - \mathbf{c}_j\|_2^2 (1)$$

Ở công thức (1) ta hiểu là: tổng sai số bằng tổng khoảng cách của tâm cụm $j$ với các điểm dữ liệu $x_i$ thuộc cụm $j$. Trong đó $j$ được hiểu ngầm là nhãn cho dữ liệu vì vậy ta sẽ có $K$ nhãn khác nhau. Công thức này sai về mặt toán học, nhưng nó khá dễ hiểu với mình để diễn đạt. Và $c_j$ là trung bình cộng của các điểm thuộc cụm $j$.

Ở đây, mình đã đơn giản hóa cách biểu diễn để khi thực code sẽ dễ dàng hơn, bạn có thể đọc thêm [tại đây](https://machinelearningcoban.com/2017/01/01/kmeans/#-phan-tich-toan-hoc) để xem cách biểu diễn dạng vector one-hot và chứng minh tâm cụm là trung bình cộng của các điểm dữ liệu thuộc cụm đó.

<a name="4-steps"></a>

## 4. Các bước giải bài toán

**Bước 1**: Chọn $K$ điểm dữ liệu làm điểm tâm cụm ban đầu. 

**Bước 2**: Tính khoảng cách mỗi điểm dữ liệu với tâm cụm và gán dữ liệu vào cụm gần nhất.

**Bước 3**: Nếu việc gán dữ liệu vào từng cluster ở bước 2 không thay đổi so với vòng lặp trước nó thì ta dừng thuật toán.

**Bước 4**: Cập nhật điểm tâm cụm bằng trung bình cộng các dữ liệu thuộc cụm đó.

**Bước 5**: Quay lại bước 2.

<a name="5-coding"><a>

## 5. Thực nghiệm với Python

```python
import numpy as np # ĐSTT
import random 
import matplotlib.pyplot as plt # Visualize
from sklearn.datasets import make_blobs # Make the dataset

# centroids
centroids = [[1, 1], [-1, -1], [1, -1]] 

# dataset
X, true_labels = make_blobs(n_samples=750, centers=centroids, 
                            cluster_std=0.4, random_state=0)

# Visualize dữ liệu
plt.plot(X[:,0],X[:,1],'o')
plt.title('Dataset')
```

<img src="/assets/images/bai6/anh4.png" class="normalpic"/>

<p align="center"> <b>Hình 4</b>: Visualize dataset</p>

Giả sử với bộ dữ liệu trên, ta khởi tạo K là số cụm và sẽ thu các tâm cụm tương ứng với giá trị ngẫu nhiên.

```python
# Khởi tạo centroids
def init_centroids(K):
    clusters = []
    K = 3
    for i in range(K):
        x = random.sample(list(X), 1)
        clusters.append(x[0])
    clusters = np.array(clusters)
    return clusters

K = 3
clusters = init_centroids(K)
plt.plot(clusters[:,0],clusters[:,1],'o')
```

<img src="/assets/images/bai6/anh5.png" class="normalpic"/>

<p align="center"> <b>Hình 5</b>: Khởi tạo tâm cụm</p>

Tiếp theo ta sẽ tính toán khoảng cách giữa mỗi điểm với các tâm cụm để cập nhật (bước 2) và sau đó cập nhật tâm cụm là trung bình cộng dữ liệu thuộc cụm đó (bước 4) cho tới khi dữ liệu không có sự thay đổi về nhãn cho 2 lần lặp liên tiếp (bước 2). 

```python
# Tính khoảng cách 2 điểm
def distance(p1,p2):
    return np.linalg.norm(p1-p2,2)

# Cập nhật nhãn cho mỗi điểm dữ liệu
def update_labels(X):
    labels = []
    for i in range(len(X)):
        fake_distance = 999999
        label = -1
        for j in range(len(clusters)):
            d = distance(X[i],clusters[j])
            if d < fake_distance:
                fake_distance = d
                label = j
        labels.append(label)
    return labels

# Cập nhật centroids dựa trên nhãn trước đó tìm được
def update_centroids(clusters,X,labels):
    for i in range(len(clusters)):
        count = 0
        x0 = 0
        y0 = 0
        for j in range(len(X)):
            if i == labels[j]:
                count += 1
                x0 += X[j][0]
                y0 += X[j][1]
        x0 /= count
        y0 /= count
        clusters[i][0] = x0
        clusters[i][1] = y0
    return labels,clusters

# Điều kiện dừng
def stop(centroids,new_centroids):
    return (set([tuple(a) for a in centroids]) == 
        set([tuple(a) for a in new_centroids]))
```

Các hàm quan trọng của bài toán đã được hoàn thành, bây giờ ta cần lắp ghép chúng vào theo phần 4 để tìm nghiệm cho bài toán. 

```python
while True:
    labels = update_labels(X)
    labels,centroids,before_centroids = update_centroids(centroids, X, labels)
    if stop(centroids,before_centroids):
        break
plt.scatter(X[:,0],X[:,1], c=labels, cmap='rainbow')
plt.scatter(centroids[:,0] ,centroids[:,1], color='black')
```

<img src="/assets/images/bai6/anh6.png" class="normalpic"/>

<p align="center"> <b>Hình 6</b>: Kết quả thu được</p>

Sau vài lần chạy, với kết quả mong muốn, ta đã chia được dữ liệu thành 3 cụm khác nhau. Lúc này khi có điểm dữ liệu mới đến, ta chỉ cần tính toán khoảng cách giữa điểm đó với các tâm cụm để lấy ra khoảng cách nhỏ nhất chính là thuộc cụm đó.

<a name="6-evaluation"></a>

## 6. Đánh giá và kết luận

- K-means là một thuật toán đơn giản, khá dễ để hiểu ý tưởng nhưng vẫn được ứng dụng khá nhiều trong các bài khác làm bước pre-processing. Hơn nữa, tuy thời gian để tính toán và suy ra nghiệm (tâm cụm) hơi tốn thời gian, nhưng bù lại khi dự đoán lại rất nhanh.

- Một số ứng dụng của K-means có thể kể tới là phân đoạn ảnh (Image Segmentation) một lĩnh vực rất hay trong Computer Vision hoặc Image Compression (nén ảnh).

- Điểm yếu của K-means đó là ta phải biết trước số cụm $K$ để phân thành $K$ dữ liệu khác nhau vì vậy có một phương pháp tên là Ebbow Method được sử dụng để chọn $K$ và thử nghiệm những giá trị $K$ phù hợp nhất có thể. 

- Một điểm yếu khác nữa chính là việc random các điểm centroids ban đầu sẽ ảnh hưởng tới kết quả của bài toán. Như ở trên, mình đã phải chạy lại vài lần để có kết quả như vậy. Thử nghĩ không may nếu ta random 3 điểm và 3 điểm này đều nằm trong 1 vùng dữ liệu thì việc cập nhật và tính toán rất khó khăn. Một cải tiến cho việc khởi tạo đó là thuật toán K-means++.

<a name="7-references"></a>

## 7. Tham khảo

[1] [Machine Learning cơ bản](https://machinelearningcoban.com/2017/01/01/kmeans/)

[2] [Machine Learning course by Andrew Ng](https://www.coursera.org/learn/machine-learning)



