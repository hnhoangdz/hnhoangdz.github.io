{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StarGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEi1z4hAxSvF"
      },
      "source": [
        "# 1. StarGAN Multi-Domain Image-to-Image translation\n",
        "\n",
        "In previous sections, you are introduced about::\n",
        "\n",
        "* [pix2pix GAN](https://phamdinhkhanh.github.io/2020/11/13/pix2pixGAN.html) is a kind of conditional GAN that you enable to control desired output. The loss function is the combination of adversarial loss and L1-loss. To train pix2pix GAN required to couple source and target images. So it is low performance on small and scarce dataset.\n",
        "\n",
        "* [CycleGAN](https://phamdinhkhanh.github.io/2021/01/20/CycleGAN.html) that is unsupervised GAN model enable to learn how to translate from one source dataset to another target dataset. CycleGAN is high performance and more realistic image generation than pix2pix because of it bases on the dataset instead of only each pair image. The model using `cycle consistent loss function` beside adversarial loss. The cycle consistent loss function is a greate creation that empower CycleGAN to auto-encode the image itself through finding out the efficient latent space that good present the $x$ input image to $y$ output image. It is pretty well studies on the small dataset without coupling image. But you only have ability to train to transfer unique domain. Such as transfer color from hourse to zibra, from colored images to sketch images or from dark to night.\n",
        "\n",
        "But how can we handle if we want to Multi-Domain translate from source to target. For example, you have a face dataset owning variety of features such as age, gender, hair color, skin face color, facial expression,.... Previous GAN like pix2pix and CycleGAN models only support to translate one feature. For example, you only enable to translate from male to female or change age from young to old. With each needed-translating feature, you have to train seperate model. Thus if you have $k$ distinct features, to transfer two features simultaneously you may develop $k*(k-1)$ GAN models. This is highly consuming cost and inefficient. Thus that is the reason to clova's AI researchers create StarGAN model to handle with problem of Multi-Domain in image-to-image translation. Replacing $k*(k-1)$ distinctive models by unique StarGAN model that is capable to learn every translation of many features through binary-encoding domain vector and input image. It is fascinating and interesting mechanism that today, we are going to discover the secret mechanism inside it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGQSAvYwANL8"
      },
      "source": [
        "# 2. How to learn Multi-Domain feature\n",
        "\n",
        "## 2.1. Problem when cross-domain\n",
        "\n",
        "You can see in image below, the cross-domain model (on the left) is only map between one to one feature. Hence, when there are four distinctive features indexed as $1, 2, 3, 4$, you have to build 12 generative model in total to cover whole translation. Such as to translate the feature `1->2` is $G_{12}$ model and `2->1` is $G_{21}$ model. It also mean that if you want to translate color of hair from `brown to black`, there is a new model is developed and to inverse translate from `black to brown`, there is another one is developed.\n",
        "\n",
        "But StarGAN on the right of the bellow image empowers to you only use unique generator architecture that is more powerful and flexiable when support to translation of multi-domain features at once. You not only translate hair color but also aligned translate with the other features such as gender or age.\n",
        "\n",
        "![](https://imgur.com/OXo5X9r.png)\n",
        "\n",
        "## 2.2. StarGAN architecture\n",
        "\n",
        "So how does the StarGAN architecture look like?\n",
        "\n",
        "![](https://imgur.com/OMmUUlf.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOS6r7k_2h-V"
      },
      "source": [
        "Model take target domain as embedding vector of condition. The idea is natural as in conditional GAN, when you want what the features are going to appear on the output, you must add condition label to input beside the input image.\n",
        "\n",
        "Abve is StarGAN generator graphic. \n",
        "\n",
        "* **At the (b) step Original-to-target domain**: Generator take both target domain and input image as the input. Target domain is binary-encoded to force generator to intentionally create the fake image with specific features. The target domain is spatially expanded to feasibly combine into input image add the new channels.\n",
        "\n",
        "* **(c) Target-to-original domain**: After step b, the fake image own the characteristic of target domain. Next, in this c step we start reconstruction process that convert target to original domain. the fake image is depth-wise concatenation with original domain to have ability to reconstruct the original features and thus, reconstruct into the original image. The whole reconstruction proccess we applied the second generator architecture that is somehow similar to CycleGAN.\n",
        "\n",
        "* **(d) Fooling the discriminator**: The discriminator after that is discriminated by discriminator. Because we not only try to create the fake image, the features is also what we want to create. Hence the discriminator is bound to classify both real/fake image and domain classification. Is that easy? I hope you understood.\n",
        "\n",
        "As ussual in the almost training discriminator, we align fake case with real case to feed into model.\n",
        "\n",
        "![](https://imgur.com/hSsew4M.png)\n",
        "\n",
        "Finally, model find the balance point that makes a good generator and discriminator to generate output images as realistic as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFQLxDPvAY_L"
      },
      "source": [
        "## 2.3. Mask Vector for Domain classification\n",
        "\n",
        "On StarGAN we train model on multi-dataset. Each dataset can own some feature that is not defined on the other dataset. For example, when we train StarGAN with face, on both CelebA and RaFD dataset. The image originates from CelebA we only have `blond hair, gender, age, pale skin` but do not have information about `angry, happy, and fearful` that defined on RaFD and vice verse. So we must apply the mask vector which forces the model to forget the features that do not have. Mask vector is exactly binary vector that encode 1 value if feature is exist and 0 in the other hand.\n",
        "\n",
        "For example: You have 7 features \n",
        "\n",
        "* blond hair: yes/no\n",
        "* gender: male 1, female 0\n",
        "* age: old 1, young 0\n",
        "* pale skin: yes/no\n",
        "* angry: yes/no\n",
        "* happy:yes/no\n",
        "* fearful: yes/no\n",
        "\n",
        "yes correspond with 1 and no correspond with 0.\n",
        "\n",
        "With one face image from CelebA it own features: `blond hair, female, young, no pale skin`. So it may be encoded as `[1, 0, 0, 0, 0, 0, 0]` that allow the order of 7 features we list above. The only CelebA's features must be defined, the RaFD's features is forgot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS_TQ6y2qqbM"
      },
      "source": [
        "# 3. The loss function\n",
        "\n",
        "The target of StarGAN is to discriminate real and fake image and classify the domain belonging to each input image. Thus discriminator return the distribution outputs include both probability of source distribution $D_{src}$ (for distingushing real/fake image) and domain distribution $D_{cls}$ (for classifying multi-domain).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzdMD74PnWjZ"
      },
      "source": [
        "## 3.1. Adversarial Loss\n",
        "\n",
        "The main adversarial loss's role is to distingush better between real and fake image. Thus, it imposes on the source distribution $D_{src}$. It actually is the kind of binary cross-entropy loss function. In which we consider the both cases contribute to loss is $y=0$ and $y=1$.\n",
        "\n",
        "$$\\mathcal{L}_{adv} = -\\mathbb{E}_{x \\sim p_{data}(x)}(\\log D_{src}(x)) - \\mathbb{E}_{z \\sim p_{data}(z), c}(1-\\log D_{src}(G(z, c)))$$\n",
        "\n",
        "$G(z, c)$ is generator suppose to generate the fake image on step (b) you see. Because of model consider the fake case as negative, hence the contribution to loss function must be $1 - \\log D_{src}(G(z, c))$. And in opposite, the real case contribution must be $\\log D_{src}(x)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7RoiTZ4oHmd",
        "outputId": "2ed67b34-2718-4a39-dfca-cb95398468d2"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def adv_loss(logits, target):\n",
        "    \"\"\"\n",
        "    logits: source probability output of discriminator.\n",
        "    target: target label of fake image. 1 as real and 0 as fake\n",
        "    \"\"\"\n",
        "    targets = torch.full_like(logits, fill_value=target)\n",
        "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
        "    return loss\n",
        "\n",
        "logits = torch.tensor([0.1])\n",
        "target = 1\n",
        "adv_loss(logits, target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6444)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW4_wqENnW8P"
      },
      "source": [
        "\n",
        "## 3.2. Domain Loss\n",
        "\n",
        "Domain Loss simply serves to optimize the Multi-Domain classification problem. The labels that Multi-Domain classifier need to predict is the target domain features of fake image or the real features from real image. In generally, Domain Loss function allowed the form crossentropy loss function:\n",
        "\n",
        "$$\\mathcal{L}_{cls} = -\\sum_{c=1}^{C} \\mathbb{E}_{x\\sim p_{data}}[\\log D_{cls}(c|x)]$$\n",
        "\n",
        "In which $D_{cls}(c|x)$ is the probability value belonging to class $c$ when the input is $x$.\n",
        "\n",
        "In case the image is real or fake also change the domain loss representation above. Intuitively, it must be:\n",
        "\n",
        "* Domain Loss Function for real image:\n",
        "\n",
        "$$\\mathcal{L}_{cls}^{r} = -\\sum_{c=1}^{C} \\mathbb{E}_{x\\sim p_{data}}[\\log D_{cls}(c|x)]$$\n",
        "\n",
        "* Domain Loss Function for fake image: The real value $x$ is replaced by output of generator $G$.\n",
        "\n",
        "$$\\mathcal{L}_{cls}^{f} = -\\sum_{c=1}^{C} \\mathbb{E}_{x\\sim p_{data}}[\\log D_{cls}(c|G(x, c))]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVz9SBZpp51C",
        "outputId": "7ab4da3b-ccb7-4cea-f89d-bd95cabb8c45"
      },
      "source": [
        "import torch\n",
        "\n",
        "def domain_loss(d_out, c):\n",
        "  \"\"\"\n",
        "  d_out: the output of descriminator\n",
        "  c: features index that fake image own.\n",
        "  \"\"\"\n",
        "  # one-hot encoding\n",
        "  v_out = torch.full_like(d_out, 0)\n",
        "  v_out[c] = 1\n",
        "  # loss function\n",
        "  loss = -v_out*torch.log(d_out)\n",
        "  loss=loss.sum()\n",
        "  return loss\n",
        "\n",
        "rand = torch.randn(4)\n",
        "d_out = torch.exp(rand)/torch.exp(rand).sum()\n",
        "c = torch.tensor([1, 2])\n",
        "\n",
        "domain_loss(d_out, c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.5890)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUj9CnAteNjS"
      },
      "source": [
        "## 3.3. Reconstruction Loss\n",
        "\n",
        "The step (b) and (c) is also very similiar to what happen in CycleGAN. In which output of process is the reconstruction of input image. We do not guarantee after applying Multi-domain translation on input image, the output is going keep the patterns of the input image. To alleviate the diffrence too much, we apply the kind of Cycle Consistency Loss under the L1-norm formulation:\n",
        "\n",
        "$$\\mathcal{L}_{rec} = \\mathbb{E}_{x, c_t, c_{org}}[||x-G_2(G_1(x, c_t), c_{org})||_{1}]$$\n",
        "\n",
        "In which $G_1$ is first generator that applied on step (b) to create fake image replying on the input $x$ and target class labels $c_t$. Then, the fake image $G_1(x, c_t)$ and added source class labels $c_{org}$ continues forwarding on $G_2$ second generator to reconstruct back to original input image.\n",
        "\n",
        "We call this loss function as reconstruction loss to remind the recovery aim of this specical loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7vnBAUxurtX",
        "outputId": "6b641cd3-96e8-487b-e9e7-710c8f77a3f2"
      },
      "source": [
        "import torch\n",
        "\n",
        "def rec_l1(x, g_out):\n",
        "  assert x.size() == g_out.size()\n",
        "  return torch.mean(torch.abs(x-g_out))\n",
        "\n",
        "x = torch.randn((4, 28, 28, 3))\n",
        "g_out = torch.randn((4, 28, 28, 3))\n",
        "\n",
        "_rec_l1(x, g_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1204)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu905MEciRju"
      },
      "source": [
        "## 3.4. Full Objective Function\n",
        "\n",
        "The full objective function is the combination of three above kinds of loss function. They are relatively assigned to loss function of discriminator and generator as bellow:\n",
        "\n",
        "$$\\mathcal{L}_{D} = \\mathcal{L}_{adv} + \\lambda_{cls} \\mathcal{L}_{cls}^{r}$$\n",
        "\n",
        "$$\\mathcal{L}_{G} = -\\mathcal{L}_{adv} + \\lambda_{cls} \\mathcal{L}_{cls}^{f} + \\lambda_{rec} \\mathcal{L}_{rec}$$\n",
        "\n",
        "You might ask to why the sign of $\\mathcal{L}_{adv}$ on generator loss? It is simple that when your generator is good enough, it intentionally create image approach to real. Thus, $D_{cls}(G(z, c))$ is approach to $1$ and that is reason cause to $\\mathcal{L}_{adv}$ is high. We revert the sign of it on generator to be adaptable with our target is to minimize loss function.\n",
        "\n",
        "Beside, $\\lambda_{cls}, \\lambda_{rec}$ are graduately two hyper-parameters of model that control the importance of domain classification and reconstruction impact on full objective loss relatively. In emperically, we choose $\\lambda_{cls}=1$ and $\\lambda_{rec}=10$.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFeCJysTJNLP"
      },
      "source": [
        "# 4.  Source code\n",
        "\n",
        "* Pytorch:\n",
        "\n",
        "https://github.com/yunjey/stargan\n",
        "\n",
        "https://github.com/clovaai/stargan-v2\n",
        "\n",
        "https://github.com/eveningglow/StarGAN-pytorch\n",
        "\n",
        "* Tensorflow:\n",
        "\n",
        "https://github.com/taki0112/StarGAN-Tensorflow\n",
        "\n",
        "https://github.com/clovaai/stargan-v2-tensorflow\n",
        "\n",
        "https://github.com/KevinYuimin/StarGAN-Tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL5Uv_8J8qLN"
      },
      "source": [
        "# 5. Conclusion\n",
        "\n",
        "Through this lesson, i introduce to you the main keypoints of StarGAN that make this model get high performance. Especially it have ability to Multi-Domain translating images that help to cost saving when required unique generator but apply variety translations. StarGAN own this extraodinary ability is depending on embedding features multi-domain features and align them with image as additional condition for inferencing.\n",
        "\n",
        "I hope you enjoy with this content and thanks for reading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XZjsZPuJE5-"
      },
      "source": [
        "# 6. Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umrDNzo0xTFs"
      },
      "source": [
        "1. [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https://arxiv.org/pdf/1711.09020.pdf)\n",
        "2. [CycleGAN - Khanhblog](https://phamdinhkhanh.github.io/2021/01/20/CycleGAN.html)\n",
        "3. [Pix2PixGAN - Khanhblog](https://phamdinhkhanh.github.io/2020/11/13/pix2pixGAN.html)\n",
        "4. [Conditional GAN - Khanhblog](https://phamdinhkhanh.github.io/2020/08/09/ConditionalGAN.html)\n",
        "5. [Wasserstein GAN - Khanhblog](https://phamdinhkhanh.github.io/2020/07/25/GAN_Wasserstein.html)"
      ]
    }
  ]
}