{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHp2HoIeje2f"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "In reality, there are many `image to image translation` tasks such as:\n",
        "\n",
        "* Transforming image color between two pictures.\n",
        "* Changing season from summer to winter.\n",
        "* Changing dark to night and night to dark.\n",
        "* Transforming colored images into sketch images.\n",
        "* Translating hourse into zebra.\n",
        "* Translating photo into the painting of the dead artists.\n",
        "\n",
        "When face up with these tasks related to image to image translation, CycleGAN may be the best facility you should think first.\n",
        "\n",
        "CycleGAN was launched in 2017 by group author `Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros`. That model find the general way to map between input images to output images without aligning the pair of source and target like the other GAN models such as Pix2Pix and CGAN. So the results gain from CycleGAN is more general when it can learn not only from source image to target image but also learn inter-domain mapping from a set of source images to target images. This model is especially useful when model is trained on scarce images that is hard to couple. Because of lacking of data, we are not easily to learn and transfer characteristic from only source image to target image. That kind of learning is unsupervised learning through unpaired data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwyDqoXIp7eN"
      },
      "source": [
        "## 1.1. Unpaired learning image to image translation\n",
        "\n",
        "We all known the normal GAN models have input is paired. That make an disavantage to model get high performance of generalization and synthesis ability. Usually model need a large scale dataset to reach generating the fake image as real. But there are some data is scarce and hard to be found, for example: the pictures from the dead artist.\n",
        "\n",
        "To overcome this disavantage, The CycleGAN's authors introduce unpaired image to image translation learning that supports translating the general characteristics from source to target set without requiring them related to in pair.\n",
        "\n",
        "![](https://imgur.com/RgdLU2Q.png)\n",
        "\n",
        "Figure 1: Paired (in the left) vs Unpaired (in the right) image to image translation. Paired training dataset exists the correspondance of each input image $x_i$ and $y_i$. In contrast, unpaired training dataset have input is both sets $X = \\{x_i\\}^{M}_{i=1}$ and $Y=\\{y_i\\}^{M}_{i=1}$ with no information of mapping $x_i$ to $y_i$ inside them.\n",
        "\n",
        "Because of unpair source-target couple, we do not exactly know what the specific source image maps to target image. But in the other hand, model learn mapping at the set level between set $X$ to set $Y$ under the unsupervised learning. The process of training corresponds with find a mapping function $G:  X \\mapsto Y$. We expect that the output is indistinguishable with real image, so the distribution of each image $x \\in X$ require $\\hat{y} = G(x)$ is identical with $y \\in Y$ and the distribution of $G(X)$ set may have to be approach to the distribution of $Y$ set. The mapping function $G$ is definitely the generator function. To train generator function, we use discriminator function help to distinguish between real and fake image. That mechanism is same the normal conditional GAN algorithm, you can refer to [cGAN](https://phamdinhkhanh.github.io/2020/08/09/ConditionalGAN.html) to more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RtsFU-E4FbQ"
      },
      "source": [
        "# 2. CycleGAN architecture\n",
        "## 2. 1. Different between CycleGAN and GAN architecture\n",
        "\n",
        "![](https://imgur.com/vGjX6DM.png)\n",
        "\n",
        "Figure2: Normal GAN architecture. Firstly, input noise vector $z$ is transform into cenerator to create fake image. discriminator help to distinguish between real/ fake data.\n",
        "\n",
        "GAN model is optimized on adversarial loss function which is the combination between generator loss and discriminator loss.\n",
        "\n",
        "$$\\min_{G} \\max_{D} V(D, G) = \\underbrace{\\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)]}_{\\text{log-probability that D predict x is real}} + \\underbrace{\\mathbb{E}_{z \\sim p_{z}(z)} [\\log (1-D(G(z)))]}_{\\text{log-probability D predicts G(z) is fake}} ~~~ (1)$$\n",
        "\n",
        "You should understand this loss function through my explanation at [adversarial loss function of GAN](https://phamdinhkhanh.github.io/2020/07/13/GAN.html#34-h%C3%A0m-loss-function). Kindly try to google translate if you are not Vietnamese.\n",
        "\n",
        "In brief, that loss function not only discriminate between real and fake data and but also learn generate fake image more realistic. But in practical, it is really hard to train only adversarial objective because of divergence and unstability. The results also is not quite good.\n",
        "\n",
        "```\n",
        "Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the well known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress.\n",
        "```\n",
        "\n",
        "Source [Unpaired Image-to-Image Translation\n",
        "using Cycle-Consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593.pdf)\n",
        "\n",
        "Thus, CycleGAN authors come up with new architecture that allow to learn from both $X$ and $Y$ set without coupling the source and target image.\n",
        "\n",
        "![](https://imgur.com/ywZJ5tX.png)\n",
        "\n",
        "Figure3: CycleGAN architecture includes two generators and discriminatorS serve to translation from $X$ to $Y$ and opposite $Y$ to $X$.\n",
        "\n",
        "This architecture also include generator and discriminator as the GAN architectures with the commission of each as bellow:\n",
        "\n",
        "* generator: Try to generate real image look realistic as possible.\n",
        "* discriminator: Improving generator through trying to discriminate real vs fake image (fake image come from generator).\n",
        "\n",
        "\n",
        "But three are some improvements of CycleGAN architecture than GAN:\n",
        "\n",
        "* We use dual generator in continous to translate $x$ to $y$ and next restrain back $y$ to $x$. This architecture create a latent space to transform image without predefine paired image.\n",
        "\n",
        "* Cycle consistency loss function is applied to keep distribution of output of dual generator process not far from input.\n",
        "\n",
        "Finally, the loss function is combination of `cycle consistency loss` and `adversarial loss`.\n",
        "\n",
        "In the next section, i explain detail more about the CycleGAN through cycle consistency loss function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAiouPVhYWEe"
      },
      "source": [
        "## 2.2. Cycle consistency loss function\n",
        "\n",
        "To overcome the hardship of collapsion when train GAN model, CycGAN propose a cycle consistency loss function that learning in both direction starting from $X$ and $Y$. We suppose another inverse function of $G$, assuming $F$ to support mapping from $Y$ set to $X$ set. That invention is proved to keep dominance to the other algorithm in image to image translation replying on `hand-defined factorizations or shared embedding functions`.\n",
        "\n",
        "As you see in figure 3, there are two generator function include $G$ on the left and $F$ in the right. $G$ help to generate $X \\mapsto Y$ and $F$ is inversed $Y \\mapsto X$. The output $G(x) \\approx y$ and being continued to forward as input of $F$. This process in brief as \n",
        "\n",
        "$$x \\mapsto G(x) \\mapsto F(G(x)) \\approx y \\tag{2}$$ \n",
        "\n",
        "And in the revert direction, we change $y$ as $x$ to get process \n",
        "\n",
        "$$y \\mapsto F(x) \\mapsto G(F(x)) \\approx y \\tag{3}$$\n",
        "\n",
        "Cycle consistency loss function try to constrain both of these $(2)$ and $(3)$ processes to it generate output become as real as possible. It corresponds with minimizing the difference between $F(G(x))-y$ and $G(F(x))-y$ reply on L1-norm as following:\n",
        "\n",
        "$$\\mathcal{L}_{cyc(G, F)} = \\mathbb{E}_{x\\sim p_{data}(x)}\n",
        "[||F(G(x)) − x||_1] + \\mathbb{E}_{y\\sim p_{data}(y)}\n",
        "[||G(F(y)) − y||_1]$$\n",
        "\n",
        "The cycle in `cycle consistency loss` meaning that the learning process repeats in both direction from $x$ and $y$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1-6cHgrdQLb"
      },
      "source": [
        "## 2.3. Adversarial loss function\n",
        "\n",
        "Adversarial loss function is the combination between generator loss and discriminator loss that totally the same as $(1)$ formulation. The aim of adversarial loss is to try simultaneously support generator generate fake image more realistic and increase discrimination capability between real and fake of discriminator.\n",
        "\n",
        "There are two process of training as in $(2), (3)$ in the cycleGAN. Thus, there are also exist two adversarial loss functions corresponding with each of them.\n",
        "\n",
        "* Start from $x$ we try to generate fake $\\hat{y}=G(x)$.\n",
        "\n",
        "$$\\mathcal{L}_{GAN}(G, D_{X} , X, Y ) = \\mathbb{E}_{y\\sim p_{data}(y)}\n",
        "[\\log D_{X} (y)]\n",
        "+ \\mathbb{E}_{x \\sim p_{data}(x)}\n",
        "[\\log(1 − D_{X} (G(x))] \\tag{4}$$\n",
        "\n",
        "* And in the revert direction, start from $y$: we try to fake $\\hat{x}=F(y)$.\n",
        "\n",
        "$$\\mathcal{L}_{GAN}(F, D_{Y} , X, Y ) = \\mathbb{E}_{x\\sim p_{data}(x)}\n",
        "[\\log D_{Y} (x)]\n",
        "+ \\mathbb{E}_{y \\sim p_{data}(y)}\n",
        "[\\log(1 − D_{Y} (F(y))] \\tag{5}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxwiK4nR0KGJ"
      },
      "source": [
        "\n",
        "## 2.4. Training strategy\n",
        "To training process stable, authors replace the negative log likelihood function $\\mathcal{L}_{GAN}$ by least-square loss. For example $\\mathbb{E}_{y\\sim p_{data}(y)}\n",
        "[\\log D_{Y} (y)]$ is replaced by $\\mathbb{E}_{y\\sim p_{data}(y)}\n",
        "[(D_{Y} (y))^2]$ and \n",
        "$\\mathbb{E}_{x \\sim p_{data}(x)}\n",
        "[\\log(1 − D_{Y} (G(x))]$ is replaced by\n",
        "$\\mathbb{E}_{x \\sim p_{data}(x)}\n",
        "[(1 − D_{Y} (G(x)))^2]$. In emperical, this way is more efficient when generate image with high quality. $\\lambda$ also set 10 is good. To reduce model oscillation, the descriminator is updated base on buffer of 50 history images instead of only lastest generator image.\n",
        "\n",
        "Finally, we got the full objective function is summation of `cycle consistency loss` and `adversarial loss`.\n",
        "\n",
        "$$\\mathcal{L}(G, F, D_X, D_Y ) =\\mathcal{L}_{GAN}(G, D_X , X, Y )\n",
        "+ \\mathcal{L}_{GAN}(F, D_Y, X, Y)\n",
        "+ \\lambda\\mathcal{L}_{cyc}(G, F)$$\n",
        "\n",
        "The training stategy is process of found the optimization:\n",
        "\n",
        "$$G^{∗}, F^{∗} = \\arg \\min_{G,F} \\max_{\n",
        "D_X,D_Y} \\mathcal{L}(G, F, D_X, D_Y )$$\n",
        "\n",
        "\n",
        "maximization is to reinforce the strength of discriminator $D_X, D_Y$ through negative log likelihood function (or cross entropy in the other name).\n",
        "minimization is reply on $G, F$ function, that try to gain the most precise mapping function to create fake image as real.\n",
        "\n",
        "Actually, we can consider model are learning autoencoder $G \\circ F$ and $F \\circ G$ jointly. One image is mapped to it-self via an intermediate representation similiar to bottle neck layer function in autoencoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvmiHwNxqdo1"
      },
      "source": [
        "# 3. Backbone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f55QwJSzbAg4"
      },
      "source": [
        "## 3.1. Generator\n",
        "The generator of CycleGAN is the convolution-deconvolution network that usually is applied in image to image translation task. In detail, the network contains three convolutions block to reduce the resolution twice time at each layer at the beginning stage. Next, we keep the resolution the same on the next 6 blocks, this is to transform these features to get the fine-grained these features at the end. And on the deconvolution, we use [transpose convolution](https://phamdinhkhanh.github.io/2020/06/10/ImageSegmention.html#7-t%C3%ADch-ch%E1%BA%ADp-chuy%E1%BB%83n-v%E1%BB%8B-transposed-convolution) to upsampling the output twice time until we meet the original resolution. I brief those architecture as bellow figure.\n",
        "\n",
        "![](https://imgur.com/xKr1czM.png)\n",
        "\n",
        "Figure4: Generator architecture include Convolutional and Deconvolutional phases. The resolution and channels are keep as above figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xIb5KYBa1tP"
      },
      "source": [
        "## 3.2. Discriminator\n",
        "\n",
        "In discriminator, the author use `70x70` patchGAN architecture to classify on each small patch, the combination of whole patches make the final result of discriminator more trusted. Such patch-level discriminator architecture also has fewer parameters than full-image discriminator.\n",
        "\n",
        "![](https://phamdinhkhanh.github.io/assets/images/20201113_Pix2Pix/pic5.png)\n",
        "\n",
        "Figure5: PatchGAN architecture, the classification result is on each patch as on the figure.\n",
        "\n",
        "About patchGAN i refer you to [PatchGAN in pix2pix](https://phamdinhkhanh.github.io/2020/11/13/pix2pixGAN.html#4-ki%E1%BA%BFn-tr%C3%BAc-pix2pix-d%E1%BB%B1a-tr%C3%AAn-patchgan)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2PB0Cp59W4V"
      },
      "source": [
        "# 4. Code\n",
        "\n",
        "The coding of cycleGAN you can find in many sources:\n",
        "\n",
        "* pytorch:\n",
        "\n",
        "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
        "\n",
        "https://github.com/aitorzip/PyTorch-CycleGAN\n",
        "\n",
        "https://github.com/yunjey/mnist-svhn-transfer\n",
        "\n",
        "* tensorflow:\n",
        "\n",
        "https://github.com/LynnHo/CycleGAN-Tensorflow-2\n",
        "\n",
        "https://www.tensorflow.org/tutorials/generative/cyclegan\n",
        "\n",
        "* mxnet:\n",
        "\n",
        "https://github.com/junyanz/CycleGAN\n",
        "\n",
        "https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGa12NOPhkrG"
      },
      "source": [
        "# 5. Conclusion\n",
        "\n",
        "CycleGAN is kind of unsuppervised GAN model learning translation from set $X$ to $Y$ without pre-defining the pair <source, target> images. CycleGAN usually used on these tasks related to image to image translation such as color transfering, object transfiguration. This architecture uses `cycle consistency loss` function enable to learn on both direction from set $X$ to $Y$ and reverse $Y$ to $X$. Through this paper, I help you clear the keypoints make CycleGAN is efficient than previous GAN architectures. Thus, you can apply CycleGAN on task such as image to image translation without ambigous understood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJLvU5bCHjR2"
      },
      "source": [
        "# 6. Reference\n",
        "\n",
        "1. [Unpaired Image-to-Image Translation using Cycle Consistent Loss](https://arxiv.org/pdf/1703.10593.pdf)\n",
        "\n",
        "2. [Augmented CycleGAN: Learning Many-to-Many Mappings\n",
        "from Unpaired Data](https://arxiv.org/pdf/1802.10151.pdf)\n",
        "\n",
        "3. https://keras.io/examples/generative/cyclegan/\n",
        "\n",
        "4. https://machinelearningmastery.com/what-is-cyclegan/\n",
        "\n",
        "5. https://junyanz.github.io/CycleGAN/\n",
        "\n",
        "6. https://jonathan-hui.medium.com/gan-cyclegan-6a50e7600d7\n",
        "\n",
        "7. https://jcheminf.biomedcentral.com/articles/10.1186/s13321-019-0404-1"
      ]
    }
  ]
}