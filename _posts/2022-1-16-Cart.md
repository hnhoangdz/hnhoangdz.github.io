---
layout: post
author: dinhhuyhoang
title: 13. Decision Tree - CART (2/2)
---

- [1. Giới thiệu](#1-introduction)
- [2. Ý tưởng chính](#2-idea)
- [3. ID3](#3-id3)
	- [3.1. Ý tưởng](#31-idea)
	- [3.2. Entropy](#32-entropy)
    - [3.3. Thuật toán ID3](#33-algorithm)
    - [3.4. Ví dụ minh họa](#34-example)
- [4. Đánh giá và kết luận](#4-evaluation)
- [5. Tham khảo](#5-references)

## 1. Giới thiệu

Ở [bài 12. Decision Tree - ID3 (1/2)](https://hnhoangdz.github.io/2022/01/13/DecisionTree.html), mình đã trình bày về thuật toán ID3 với hàm _information gain_ dùng làm điều kiện để đưa ra quyết định chọn thuộc tính khi xây dựng một cây quyết định. Thuật toán này sẽ chỉ làm việc với dữ liệu dạng _categorical_ (có thể chuyển từ _numeric_ sang _categorical_) tuy nhiên như đã bàn luận thì cách này có thể làm mất đi tính quan trọng của dữ liệu. Vì vậy, ở bài này mình sẽ trình bày về thuật toán có thể làm việc với cả dữ liệu _categorical_ và liên tục, đó là: _Classification And Regression Tree (CART)._

CART sẽ được xây dựng một Decision Tree bằng cách ở mỗi **node** chỉ tạo ra 2 **child node** có nghĩa rằng ở thuật toán này, cây được xây dựng sẽ là một cây nhị phân (binary tree). Vì vậy các câu hỏi lúc này sẽ trở thành dạng _True, False_, hơn nữa với CART sẽ sử dụng một độ đo khác để tính toán độ tinh khiết của phân phối xác xuất tại **non-leaf node** dễ dàng hơn đó là Gini. Về phần ý tưởng thì thuật toán _ID3_ và _CART_ đều sử dụng Greedy để tìm ra những thuộc tính phù hợp nhất ở mỗi lần lặp để đặt vào **non-leaf node** với một chỉ số đánh giá mức độ tinh khiết lần lượt là: _information gain_ và _gini_. Chi tiết mình sẽ trình bày bên dưới.

## 2. Ví dụ minh họa

Để dễ dàng cho việc giải thích ở một số vấn đề chuyên sâu thì mình sẽ sử dụng thư viện _sklearn_ để biểu diểu về mặt kết quả trước, sau đó mình sẽ giải thích sau. Cùng với bộ dữ liệu về thời tiết đã được sử dụng ở [bài 12. Decision Tree - ID3 (1/2)](https://hnhoangdz.github.io/2022/01/13/DecisionTree.html), tuy nhiên mình cần tiền xử lí một chút vì Decision Tree trong _sklearn_ yêu cầu đầu vào phải là dạng số. Ta cần xử lí như sau:

```python
import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/tiepvupsu/DecisionTreeID3/master/weather.csv')
df = df.drop(columns=["id"])
d = {
    'outlook': {'sunny': 0, 'overcast': 1, 'rainy': 2},
    'temperature': {'hot': 0, 'mild': 1, 'cool': 2},
    'humidity': {'high': 0, 'normal': 1},
    'wind': {'weak': 0, 'strong': 1},
    'play': {'no': 0, 'yes': 1}
}
for k in d:
  df[k] = df[k].map(d[k])
```

<hr>

<img src="/assets/images/bai13/anh2.png" class="median"/>

<hr>

Tiếp theo ta cần xác định dữ liệu là thuộc tính và biến mục tiêu:

```python
features = ['outlook', 'temperature', 'humidity', 'wind']
target = 'play'

X = df[features]
y = df[target]
```

Cuối cùng mình sẽ import các thư viện cần thiết của sklearn để tạo nên cây quyết định dựa trên thuật toán _CART_: 

```python
import graphviz
from sklearn import tree
tree_clf.fit(X, y)
dot_data = tree.export_graphviz(tree_clf, out_file=None, 
                                feature_names=df.iloc[:,:-1].columns,  
                                class_names=['0','1'],
                                rounded=True,
                                filled=True)

graph = graphviz.Source(dot_data, format="png")
graph
```

<hr>

<img src="/assets/images/bai13/anh3.png" class="median"/>

<hr>

Như đã đề cập bên trên thì thuật toán _CART_ sẽ tạo ra một cây nhị phân. Nếu bạn để ý thì các **leaf-node** lúc này đều có một giá trị `gini = 0.0` đây chính là cách mà thuật toán _CART_ lựa chọn thuộc tính (chi tiết mình sẽ trình bày bên dưới). Với cây ta đã tạo được, đảm bảo rằng kết quả dự đoán trên training data sẽ chính xác 100% giống với ID3. Bây giờ nếu cần dự đoán một dữ liệu mới ta chỉ cần đi hỏi liên tiếp các câu hỏi cho tới khi **non-leaf node** xuất hiện, cụ thể mình sẽ kiểm tra như sau:

- Cố định 2 thuộc tính `humidity = 1, wind = 0`

- Các thuộc tính khác sẽ được chọn ngẫu nhiên

- Đưa ra dự đoán 10 lần với cách lựa chọn trên 

```python
for i in range(10):
  outlook = np.random.randint(0, 3)
  temperature = np.random.randint(0, 3)
  x = pd.DataFrame({'outlook':outlook, 'temperature':temperature, 'humidity':[1], 'wind':[0]})

  y_pred = tree_clf.predict_proba(x)[0]
  label = tree_clf.predict(x)[0]

  print('probability low {}, high {}'.format(y_pred[0], y_pred[1]))
  print('predicted label {}'.format(label))
```

**Kết quả hoàn toàn giống nhau:**

```python
probability low 0.0, high 1.0
predicted label 1
probability low 0.0, high 1.0
predicted label 1
probability low 0.0, high 1.0
predicted label 1
probability low 0.0, high 1.0
predicted label 1
probability low 0.0, high 1.0
predicted label 1
probability low 0.0, high 1.0
predicted label 1
probability low 0.0, high 1.0
predicted label 1
probability low 0.0, high 1.0
predicted label 1
probability low 0.0, high 1.0
predicted label 1
probability low 0.0, high 1.0
predicted label 1
```

Điều này là hoàn toàn chính xác, nếu bạn nhìn lại cây mà ta đã tạo được thì dự đoán này sẽ chỉ đi theo 1 nhánh màu xanh như sau:

<hr>

<img src="/assets/images/bai13/anh4.png" class="median"/>

<hr>

Tới đây là cách cơ bản để tạo một Decision Tree dựa trên thuật toán _CART_ trong sklearn. Nếu bạn chưa hiểu cơ bản cách Decision Tree hoạt động và tạo nên như thế nào thì bạn có thể đọc lại bài [bài 12. Decision Tree - ID3 (1/2)](https://hnhoangdz.github.io/2022/01/13/DecisionTree.html) mình đã trình bày chi tiết. Vậy với Decision Tree được tạo nên bởi thuật toán _CART_ sử dụng độ đo tinh khiết là _Gini_ được làm như thế nào? Mình sẽ trình bày cụ thể bên dưới.

## 3. Thuật toán CART

Để hiểu rõ mặt bản chất của phương pháp sử dụng _Gini_ làm thước đo độ tinh khiết, ở phần này mình sẽ trình bày lại một chút về mặt toán học (bất đẳng thức) một vấn đề rất hay của bậc THCS. 

### 3.1. Bất đẳng thức Cauchy - Schwarz (Bunyakovsky)

Cho các số thực $(a_1, a_2, a_3, ..., a_n)$ và $(b_1, b_2, b_3, ..., b_n)$ với $n \geq 2$, ta có bất đẳng thức sau:

$$(a_1^2 + a_2^2 + a_3 + ... + a_n^2)(b_1^2 + b_2^2 + b_3^2 + ... + b_n^2) \geq (a_1b_1 + a_2b_2 + a_3b_3 + ... + a_nb_n)^2$$

**Chứng minh:**

Đặt: 

$$A = a_1^2 + a_2^2 + a_3 + ... + a_n^2$$

$$B = b_1^2 + b_2^2 + b_3 + ... + b_n^2$$

$$C = a_1b_1 + a_2b_2 + a_3b_3 + ... + a_nb_n$$

Xét giá trị $x$ là một số thực bất kì ta luôn có:

$$(a_1x - b_1)^2 \geq 0$$

$$<=> a_1^2x^2 - 2a_1xb_1 + b_1^2 \geq 0$$

Tương tự vẫn xét giá trị $x$ này với các giá trị $(a_1, a_2, ... a_n)$ và $(b_1, b_2,... b_n)$, rồi cộng vế ta được:

$$(a_1^2 + a_2^2 + a_3 + ... + a_n^2)x^2 - 2x(a_1b_1 + a_2b_2 + ... + a_nb_n) + (b_1^2 + b_2^2 + b_3 + ... + b_n^2)  \geq 0 \quad \quad (1)$$

Xét giá trị $x = \frac{C}{A}$ và thế $x, A, B, C$ vào bất đẳng (1) suy ra:

$$A\frac{C^2}{A^2} - 2\frac{C^2}{A} + B \geq 0$$

$$<=> B - \frac{C^2}{A} \geq 0$$

$$<=> AB \geq C^2 (\text{dpcm})$$

Xét với $b_1 = b_2 = b_3 = ... = b_n = 1$ thì bất đẳng thức lúc này trở thành: 

$$(a_1^2 + a_2^2 + a_3 + ... + a_n^2)n \geq (a_1 + a_2 + a_3 + ... + a_n)^2  \quad \quad (2)$$

Ngoài ra có một bất đẳng thức:

$$a_1^2 + a_2^2 + a_3 + ... + a_n^2 \leq (a_1 + a_2 + a_3 + ... + a_n)^2 \quad \quad (3)$$

### 3.2. Gini index

_Gini index_ tương tự như _information gain_ mà mình đã trình bày ở [bài 12. Decision Tree - ID3 (1/2)](https://hnhoangdz.github.io/2022/01/13/DecisionTree.html), dùng để đánh giá xem việc phân chia ở **node** điều kiện có tốt hay không. Tuy nhiên, đầu tiên ta sẽ cần tính toán chỉ số  _Gini_ ở mỗi **node**. 

**Bước 1**: Xét bài toán phân loại có $C$ class, node đang tới là một **non-leaf node** $\mathcal{p}$ với các điểm dữ liệu tạo thành là $\mathcal{S}$ có số lượng phần tử là $N$. $N_i$ là số lượng phần tử của class $i$ với $i = 1, 2, ..., C$. Xác suất để mỗi điểm dữ liệu rơi vào một class $i$ được xấp xỉ bằng $\frac{N_i}{N} = p_i => \sum_{i=1}^C p_i = 1$. Chỉ số này được tính bằng cách lấy 1 trừ đi tổng bình phương phân phối xác suất ở mỗi lớp, như sau:

$$\mathbf{\text{Gini}(\mathcal{S})} = 1-\sum_{i=1}^{C} p_i^2  \quad\quad (4)$$

Dựa vào (3) suy ra: $\sum_{i=1}^{C}p_i^2 \leq (\sum_{i=1}^{C} p_i)^2 =>\sum_{i=1}^{C}p_i^2 \leq 1 \quad  (4) => \mathbf{\text{Gini}(\mathbf{S})} \geq 0$ và dấu '=' xảy ra khi và chỉ khi tồn tại một giá trị $p_i = 1$ (nhãn thuộc về một lớp duy nhất).

Dựa vào (2) suy ra: $\sum_{i=1}^{C}p_i^2 \geq \frac{(\sum_{i=1}^{C} p_i)^2}{C} =>\sum_{i=1}^{C}p_i^2 \geq \frac{1}{C} => \mathbf{\text{Gini}(\mathbf{S})} \leq \frac{C-1}{C}$ và dấu '=' xảy ra khi và chỉ khi $p_i = \frac{1}{C}$ với mọi $i = 1, 2, ..., C$ (phân phối nhãn của các lớp đang chia đều).

**Bước 2**: Sau khi đã tính toán được giá trị _Gini_ cho **non-leaf node** $\mathcal{p}$, ta cần chọn ra một thuộc tính $x$. Và khi $x$ được chọn thì nhánh tiếp theo sẽ có số lượng **child node** bằng với số giá trị mà thuộc tính $x$ có. Dựa trên $x$, các điểm dữ liệu trong $\mathcal{S}$ được phân ra thành $K$ child node $\mathcal{S_1},\mathcal{S_2},…,\mathcal{S_K}$ với số điểm trong mỗi **child node** lần lượt là $m_1,m_2,…,m_K$. Suy ra tổng trọng số _Gini_ của mỗi **child node** là:

$$\mathbf{\text{Gini}}(x, \mathcal{S}) = \sum_{k=1}^K \frac{m_k}{N} \mathbf{\text{Gini}}(\mathcal{S}_k) \quad\quad (5)$$

Thông số đánh giá _Gini index_ dựa trên thuộc tính $x$ được tính toán như sau:

$$\mathbf{\text{G}}(x, \mathcal{S}) = \mathbf{\text{Gini}(\mathcal{S})} - \mathbf{\text{Gini}}(x, \mathcal{S})$$

Cuối cùng, thuộc tính được lựa chọn $x^*$ là:

$$x^* = \arg\max_{x} \mathbf{G}(x, \mathcal{S}) = \arg\min_{x} \mathbf{\text{Gini}}(x, \mathcal{S}) $$

tức $x^*$ làm cho _Gini index_ **lớn nhất** hoặc tổng trọng số _Gini_ **nhỏ nhất**

**Bước 3**: Bước 1 và Bước 2 được lặp lại cho tới khi cây tạo ra có thể dự đoán chính xác 100% training data hoặc toàn bộ thuộc tính đã được xét tới. Tuy nhiên cách dừng thuật toán này có thể gây ra Overfiting, ở phần sau mình sẽ bàn luận chi tiết hơn. Chú ý: mỗi thuộc tính chỉ được xuất hiện một lần trên một nhánh.

_Nếu bạn đã đọc [bài 12. Decision Tree - ID3 (1/2)](https://hnhoangdz.github.io/2022/01/13/DecisionTree.html) thì cách 2 độ đo information gain và gini index làm giống nhau._ 

Tuy nhiên, như đã đề cập ở phần đầu giới thiệu thì thuật toán _CART_ của thư viện sklearn sẽ chỉ sử dụng một binary tree để tạo nên Decision Tree. Vậy làm sao để với một thuộc tính bao gồm nhiều hơn 2 giá trị có thể chia thành 2 **child node**? Ta chỉ cần xét một ngưỡng $t$ để phân tách thành giá trị thuộc tính làm 2 vùng nhỏ hơn $t$ và lớn hơn $t$. Vì vậy ở bước công thức (5), cây lúc này sẽ là cây nhị phân nên có thể viết lại như sau với thuật toán _CART_ trong sklearn:

Một ngưỡng $t$ sẽ phân chia tập $\mathcal{S}$ thành 2 nửa: $\mathcal{S_0}$ có số lượng phần tử là $N_0$, $\mathcal{S_1}$ có số lượng phần tử là $N_1$

$$\mathbf{\text{Gini}}(x, t; \mathcal{S}) = \frac{N_0}{N}\mathbf{\text{Gini}}(\mathcal{S}_0) + \frac{N_1}{N} \mathbf{\text{Gini}}(\mathcal{S}_1)$$

tới đây ta chỉ cần tuning giá trị $t$ với thuộc tính $x$ để tìm ra giá trị hàm _Gini_ **nhỏ nhất** và đó chính là thuộc tính và ngưỡng mà sẽ xét làm **non-leaf node**. Và đây chính là hàm _cost function_ của thuật toán _CART_ trong bài toán phân loại. 

_Đọc tới đây chắc hẳn bạn đã hiểu cách một cây nhị phân được tạo ra như thế nào để fit chính xác 100% ở tập dữ liệu trên phần 2._

## 4. Ý nghĩa các node trong sklearn

<img src="/assets/images/bai13/anh5.png" class="median"/>

Ở **node** trên cùng chính là **root node**, các giá trị được biểu diễn như sau:

- `humidity <= 0.5`: điều kiện phân tách tại node đang xét

- `gini = 0.459`: chỉ số gini của node tại node đang xét - công thức (4)

- `samples = 14`: tổng số samples tại node đang xét

- `value = [5,9]`: số lượng phân phối theo class (#class = 0: 5, #class = 1: 9)

- `class = 1`: voting class có phân phối lớn nhất làm giá trị dự đoán (nếu cần)

Và sau khi phân tách, số samples ở **non-leaf node** bằng tổng samples 2 **child node** tạo ra. Tuy nhiên tại sao lại cần thêm 1 bước voting class khi ta dự đoán đã chính xác 100% trên tập training? Câu trả lời đơn giản rằng, nếu để một cây quyết định tạo ra tới khi hoàn toàn tinh khiết ở **leaf node** thì rất rất có thể xảy ra hiện tượng _Overfitting_. Vì vậy, cách đơn giản nhất mà ta có thể nghĩ ngay tới dừng độ sâu của cây ở một mức nào đó và sử dụng voting để đưa ra dự đoán. Ngoài ra còn một số cách khác sẽ được trình bày bên dưới. 

## 5. Xử lí Overfiting

Nói chung trong một Decision Tree nếu cứ để cây có thể tạo ra cho tới khi toàn bộ **child node** trở thành **leaf node** sẽ xảy ra hiện tượng _Overfiting_. Và càng sâu thì số lượng mỗi điểm dữ liệu sẽ càng nhỏ ở mỗi **non-leaf node**. Dựa vào đây có 2 cách để giảm thiểu Overfitting đó là đưa ra một điều kiện dừng và cắt tỉa (Pruning).

### 5.1. Điều kiện dừng

Cây sẽ không tiếp tục tạo mà sẽ dừng khi gặp một số điều kiện sau:

- Giới hạn độ sâu của cây (đã đề cập bên trên).

- Số lượng samples của một **node** đạt một ngưỡng tối thiểu. Trong trường hợp này, ta chấp nhận có một số điểm bị phân lớp sai để tránh overfitting. Class cho **leaf node** này có thể được xác định dựa trên voting. Với phân loại nhị phân thì 30 samples là đủ tin cậy.

- Giới hạn số lượng tất cả các loại **node** được tạo ra.

- Giới hạn số lượng **leaf node**.

- _information gain_ hoặc _gini index_ đạt một ngưỡng đủ nhỏ (không giảm hoặc giảm ít).

Trong class `DecisionTreeClassifier` cung cấp một số tham số với giá trị _default_ để điều chỉnh điều kiện dừng như sau:

```
DecisionTreeClassifier(*, 
criterion='gini', 
max_depth=None, 
min_samples_split=2, 
min_samples_leaf=1,  
max_features=None, 
max_leaf_nodes=None, 
min_impurity_decrease=0.0,
)
```

trong đó:

  - `criterion='gini'`: Hàm đo độ tinh khiết. Có thể là gini hoặc entropy
  - `max_depth=None`: Độ sâu tối đa cho một cây quyết định
  - `min_samples_split=2`: Số lượng samples của một node đạt một ngưỡng tối thiểu
  - `min_samples_leaf=1`: Số lượng các node lá tối thiểu của cây quyết định.
  - `max_features=None`: Số lượng các thuộc tính được thử và chọn ở bước tìm thuộc tính tốt nhất 
  - `max_leaf_nodes=None`: Số lượng các node lá tối đa của cây quyết định.
  - `min_impurity_decrease=0.0`: Tiếp tục phân chia một node nếu như sự suy giảm của độ tinh khiết nếu phân chia lớn hơn ngưỡng này  

### 5.2 Pruning